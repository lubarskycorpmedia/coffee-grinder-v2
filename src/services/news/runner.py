# /src/services/news/runner.py

import json
import fcntl
import time
import os
import asyncio
from typing import Dict, Any, Optional, Callable, Union
from datetime import datetime, timezone
from contextlib import contextmanager
from pathlib import Path

import redis
from pydantic import BaseModel, ValidationError

from src.config import get_news_providers_settings, get_ai_settings, get_google_settings
from src.logger import setup_logger
from src.services.news.pipeline import NewsPipelineOrchestrator


class NewsProviderConfig(BaseModel):
    """–°—Ö–µ–º–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    query: Optional[str] = None
    category: Optional[str] = None
    published_at: Optional[str] = None
    from_date: Optional[str] = None
    to_date: Optional[str] = None
    language: Optional[str] = "en"
    limit: Optional[int] = 50
    country: Optional[str] = None
    timeframe: Optional[str] = None


class ProcessingConfig(BaseModel):
    """–ü–æ–ª–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    providers: Dict[str, NewsProviderConfig]
    
    @classmethod
    def from_json_file(cls, file_path: str) -> "ProcessingConfig":
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Configuration file not found: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–æ—Ä–Ω–µ–≤—ã–µ –∫–ª—é—á–∏ –≤ providers
        providers = {}
        for provider_name, config_data in data.items():
            providers[provider_name] = NewsProviderConfig(**config_data)
        
        return cls(providers=providers)


class ProgressTracker:
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ Redis"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url, decode_responses=True)
        self.key = "news_processing_progress"
        
    def update_progress(self, 
                       state: str,
                       percent: int = 0,
                       current_provider: Optional[str] = None,
                       message: str = "",
                       processed_providers: Optional[list] = None,
                       start_time_override: Optional[float] = None):
        """–û–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ Redis"""
        current_time = datetime.now(timezone.utc)
        progress_data = {
            "state": state,  # idle|running|completed|error
            "percent": percent,
            "current_provider": current_provider or "",
            "processed_providers": json.dumps(processed_providers or []),
            "message": message,
            "timestamp": current_time.isoformat(),
        }
        
        current_progress = self.get_progress()
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
        if state == "running":
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω start_time_override - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            if start_time_override is not None:
                override_time = datetime.fromtimestamp(start_time_override, timezone.utc)
                progress_data["start_time"] = override_time.isoformat()
            else:
                # –ï—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å—Ç–∞—Ç—É—Å –±—ã–ª completed/error/idle - —ç—Ç–æ –Ω–æ–≤—ã–π –∑–∞–ø—É—Å–∫
                prev_state = current_progress.get("state", "idle")
                if prev_state in ["completed", "error", "idle"]:
                    # –ù–æ–≤—ã–π –∑–∞–ø—É—Å–∫ - —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–æ–≤–æ–µ –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
                    progress_data["start_time"] = current_time.isoformat()
                else:
                    # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π start_time
                    progress_data["start_time"] = current_progress.get("start_time", current_time.isoformat())
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è –∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        if state in ["completed", "error"] and "start_time" in current_progress:
            progress_data["end_time"] = current_time.isoformat()
            
            # –í—ã—á–∏—Å–ª—è–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            try:
                start_time_str = current_progress["start_time"]
                start_time = datetime.fromisoformat(start_time_str.replace("Z", "+00:00"))
                duration_seconds = (current_time - start_time).total_seconds()
                progress_data["duration"] = round(duration_seconds, 2)
            except (ValueError, KeyError) as e:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
                pass
        
        self.redis_client.hset(self.key, mapping=progress_data)
        
    def get_progress(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å"""
        data = self.redis_client.hgetall(self.key)
        if not data:
            return {
                "state": "idle",
                "percent": 0,
                "current_provider": "",
                "processed_providers": [],
                "message": "–ì–æ—Ç–æ–≤ –∫ –∑–∞–ø—É—Å–∫—É",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º processed_providers –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫
        if "processed_providers" in data:
            try:
                data["processed_providers"] = json.loads(data["processed_providers"])
            except (json.JSONDecodeError, TypeError):
                data["processed_providers"] = []
        
        return data
    
    def clear_progress(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å"""
        self.redis_client.delete(self.key)


@contextmanager
def file_lock(lock_file: str, timeout: int = 300):
    """
    –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ñ–∞–π–ª–æ–≤–æ–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
    
    Args:
        lock_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        timeout: –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    """
    lock_path = Path(lock_file)
    lock_path.parent.mkdir(parents=True, exist_ok=True)
    
    start_time = time.time()
    
    while True:
        try:
            with open(lock_file, 'w') as f:
                fcntl.flock(f.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
                f.write(f"PID: {os.getpid()}\nStarted: {datetime.now(timezone.utc).isoformat()}\n")
                f.flush()
                
                try:
                    yield
                finally:
                    fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                break
                
        except IOError:
            if time.time() - start_time > timeout:
                raise RuntimeError(f"Could not acquire lock within {timeout} seconds. Process may be already running.")
            time.sleep(1)


def run_news_parsing_from_config(
    config_path: str = "data/news_parsing_config.json",
    progress_callback: Optional[Callable] = None,
    test_without_export: bool = False,
    redis_url: str = "redis://localhost:6379"
) -> Dict[str, Any]:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
    
    Args:
        config_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        progress_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        test_without_export: –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Google Sheets
        redis_url: URL Redis –¥–ª—è tracking –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    # –ó–ê–ú–ï–† –í–†–ï–ú–ï–ù–ò –ù–ê–ß–ò–ù–ê–ï–¢–°–Ø –°–†–ê–ó–£ –ü–†–ò –ó–ê–ü–£–°–ö–ï
    start_time = time.time()
    
    logger = setup_logger(__name__)
    progress_tracker = ProgressTracker(redis_url)
    lock_file = "/tmp/news_processing.lock"
    
    try:
        with file_lock(lock_file):
            logger.info("üîê –ü–æ–ª—É—á–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            try:
                config = ProcessingConfig.from_json_file(config_path)
                logger.info(f"üìù –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ {config_path}")
                logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤: {list(config.providers.keys())}")
            except (FileNotFoundError, ValidationError, json.JSONDecodeError) as e:
                error_msg = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {str(e)}"
                logger.error(error_msg)
                progress_tracker.update_progress("error", 0, message=error_msg)
                return {"success": False, "error": error_msg}
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            progress_tracker.update_progress(
                "running", 
                0, 
                message="–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏...",
                start_time_override=start_time
            )
            
            total_providers = len(config.providers)
            processed_providers = []
            all_results = {}
            
            for i, (provider_name, provider_config) in enumerate(config.providers.items()):
                current_percent = int((i / total_providers) * 100)
                progress_tracker.update_progress(
                    "running",
                    current_percent,
                    current_provider=provider_name,
                    processed_providers=processed_providers,
                    message=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ {provider_name}..."
                )
                
                if progress_callback:
                    progress_callback(current_percent, provider_name)
                
                logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {provider_name}")
                
                try:
                    # –°–æ–∑–¥–∞–µ–º –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
                    orchestrator = NewsPipelineOrchestrator(provider=provider_name)
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                    config_dict = provider_config.model_dump(exclude_none=True)
                    query = config_dict.get("query", "")
                    category = config_dict.get("category", "")
                    limit = config_dict.get("limit", 50)
                    language = config_dict.get("language", "en")
                    from_date = config_dict.get("from_date")
                    to_date = config_dict.get("to_date")
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º pipeline (NOTE: test_without_export –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏)
                    result = orchestrator.run_pipeline(
                        query=query,
                        categories=[category] if category else [],
                        limit=limit,
                        language=language,
                        from_date=from_date,
                        to_date=to_date
                    )
                    all_results[provider_name] = {"success": result.success, "result": result}
                    
                    if result.success:
                        logger.info(f"‚úÖ –ü—Ä–æ–≤–∞–π–¥–µ—Ä {provider_name} –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                    else:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ {provider_name}: {result.errors}")
                    
                    processed_providers.append(provider_name)
                    
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {provider_name}: {str(e)}"
                    logger.error(error_msg)
                    all_results[provider_name] = {"success": False, "error": error_msg}
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            total_success = all([r.get("success", False) for r in all_results.values()])
            final_state = "completed" if total_success else "error"
            
            progress_tracker.update_progress(
                final_state,
                100,
                processed_providers=processed_providers,
                message="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞" if total_success else "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–∞–º–∏"
            )
            
            if progress_callback:
                progress_callback(100, None)
            
            logger.info("üèÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            
            return {
                "success": total_success,
                "providers_processed": len(processed_providers),
                "total_providers": total_providers,
                "results": all_results
            }
            
    except RuntimeError as e:
        error_msg = str(e)
        logger.error(f"üîí {error_msg}")
        progress_tracker.update_progress("error", 0, message=error_msg)
        return {"success": False, "error": error_msg}
    
    except Exception as e:
        error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        logger.error(f"üí• {error_msg}")
        progress_tracker.update_progress("error", 0, message=error_msg)
        return {"success": False, "error": error_msg}


def run_news_parsing_sync(
    config_path: str = "data/news_parsing_config.json",
    test_without_export: bool = False,
    redis_url: str = "redis://localhost:6379"
) -> Dict[str, Any]:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ cron
    """
    return run_news_parsing_from_config(
        config_path=config_path,
        test_without_export=test_without_export,
        redis_url=redis_url
    )


 